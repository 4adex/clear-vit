[07/15 13:32:33] detectron2 INFO: Rank of current process: 0. World size: 1
[07/15 13:32:34] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hawkeye/works/clear-vit/object_detection/detectron2
Compiler                         GCC 13.3
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.7.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 5070 Ti (arch=12.0)
Driver version                   570.133.20
CUDA_HOME                        /usr
Pillow                           11.3.0
torchvision                      0.22.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120;-gencode;arch=compute_120,code=compute_120
  - CuDNN 90.7.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=12.8, CUDNN_VERSION=9.7.1, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[07/15 13:32:34] detectron2 INFO: Command line arguments: Namespace(config_file='configs/dino-vitdet/dino_vitdet_base_modified.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=[])
[07/15 13:32:34] detectron2 INFO: Contents of args.config_file=configs/dino-vitdet/dino_vitdet_base_modified.py:
[38;5;245m# train_simple_vit.py[39m

[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mdino_vitdet_modified[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;245m# Common defaults[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/coco_detr.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mdataloader[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mAdamW[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mlr_multiplier_12ep[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mtrain[39m

[38;5;245m# Custom training parameters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;245m# No pretrained weights[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/dino_simple_vit[39m[38;5;186m"[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mcheckpointer[39m[38;5;204m.[39m[38;5;15mperiod[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mmax_norm[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mnorm_type[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcuda[39m[38;5;186m"[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m

[38;5;245m# Optimizer config[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mbetas[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0.9[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m0.999[39m[38;5;15m)[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;204min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;245m# Dataloader config[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m

[07/15 13:32:34] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/dino_simple_vit/config.yaml is human-readable but cannot be loaded.
[07/15 13:32:34] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/dino_simple_vit/config.yaml.pkl.
[07/15 13:32:34] detectron2 INFO: Full config saved to ./output/dino_simple_vit/config.yaml
[07/15 13:32:34] d2.utils.env INFO: Using a generated random seed 34946078
[07/15 13:32:34] detectron2 INFO: Model:
DINO(
  (backbone): SimpleFeaturePyramid(
    (simfp_3): Sequential(
      (0): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): SimpleViT(
      (vit): SimpleVisionTransformer(
        (conv_proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
        (encoder): Encoder(
          (dropout): Dropout(p=0.1, inplace=False)
          (layers): Sequential(
            (encoder_layer_0): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_1): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_2): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_3): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_4): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_5): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_6): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_7): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_8): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_9): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_10): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_11): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        )
        (heads): Sequential(
          (head): Linear(in_features=192, out_features=0, bias=True)
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (position_embedding): PositionEmbeddingSine()
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0-3): 4 x ConvNormAct(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  (transformer): DINOTransformer(
    (encoder): DINOTransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DINOTransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (class_embed): ModuleList(
        (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-6): 7 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (class_embed): ModuleList(
    (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
  )
  (bbox_embed): ModuleList(
    (0-6): 7 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (criterion): Criterion DINOCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 1, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0}
      num_classes: 80
      eos_coef: None
      focal loss alpha: 0.25
      focal loss gamma: 2.0
  (label_enc): Embedding(80, 256)
)
[07/15 13:32:39] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 5.11 seconds.
[07/15 13:32:39] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/15 13:32:42] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/15 13:32:43] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/15 13:32:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/15 13:32:43] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/15 13:32:44] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/15 13:32:44] d2.data.build INFO: Making batched data loader with batch_size=16
[07/15 13:32:45] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/15 13:32:45] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/15 13:32:45] d2.engine.train_loop INFO: Starting training from iteration 0
[07/15 13:32:45] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/hawkeye/works/clear-vit/object_detection/dino/train_net.py", line 105, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/dino/modeling/dino.py", line 189, in forward
    features = self.backbone(images.tensor)  # output feature dict
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detrex/detrex/modeling/backbone/eva.py", line 583, in forward
    bottom_up_features = self.net(x)
                         ^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 283, in forward
    assert h % patch_size == 0 and w % patch_size == 0, "Image size must be divisible by patch size"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Image size must be divisible by patch size
[07/15 13:32:45] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[07/15 13:32:45] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 391M
[07/15 13:40:04] detectron2 INFO: Rank of current process: 0. World size: 1
[07/15 13:40:04] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hawkeye/works/clear-vit/object_detection/detectron2
Compiler                         GCC 13.3
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.7.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 5070 Ti (arch=12.0)
Driver version                   570.133.20
CUDA_HOME                        /usr
Pillow                           11.3.0
torchvision                      0.22.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120;-gencode;arch=compute_120,code=compute_120
  - CuDNN 90.7.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=12.8, CUDNN_VERSION=9.7.1, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[07/15 13:40:04] detectron2 INFO: Command line arguments: Namespace(config_file='configs/dino-vitdet/dino_vitdet_base_modified.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=[])
[07/15 13:40:04] detectron2 INFO: Contents of args.config_file=configs/dino-vitdet/dino_vitdet_base_modified.py:
[38;5;245m# train_simple_vit.py[39m

[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mdino_vitdet_modified[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;245m# Common defaults[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/coco_detr.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mdataloader[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mAdamW[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mlr_multiplier_12ep[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mtrain[39m

[38;5;245m# Custom training parameters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;245m# No pretrained weights[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/dino_simple_vit[39m[38;5;186m"[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mcheckpointer[39m[38;5;204m.[39m[38;5;15mperiod[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mmax_norm[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mnorm_type[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcuda[39m[38;5;186m"[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m

[38;5;245m# Optimizer config[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mbetas[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0.9[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m0.999[39m[38;5;15m)[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;204min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;245m# Dataloader config[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m

[07/15 13:40:04] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/dino_simple_vit/config.yaml is human-readable but cannot be loaded.
[07/15 13:40:04] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/dino_simple_vit/config.yaml.pkl.
[07/15 13:40:04] detectron2 INFO: Full config saved to ./output/dino_simple_vit/config.yaml
[07/15 13:40:04] d2.utils.env INFO: Using a generated random seed 5332339
[07/15 13:40:04] detectron2 INFO: Model:
DINO(
  (backbone): SimpleFeaturePyramid(
    (simfp_3): Sequential(
      (0): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): SimpleViT(
      (vit): SimpleVisionTransformer(
        (conv_proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
        (encoder): Encoder(
          (dropout): Dropout(p=0.1, inplace=False)
          (layers): Sequential(
            (encoder_layer_0): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_1): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_2): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_3): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_4): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_5): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_6): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_7): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_8): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_9): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_10): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_11): EncoderBlock(
              (ln_1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=192, out_features=768, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=768, out_features=192, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        )
        (heads): Sequential(
          (head): Linear(in_features=192, out_features=0, bias=True)
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (position_embedding): PositionEmbeddingSine()
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0-3): 4 x ConvNormAct(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  (transformer): DINOTransformer(
    (encoder): DINOTransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DINOTransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (class_embed): ModuleList(
        (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-6): 7 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (class_embed): ModuleList(
    (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
  )
  (bbox_embed): ModuleList(
    (0-6): 7 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (criterion): Criterion DINOCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 1, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0}
      num_classes: 80
      eos_coef: None
      focal loss alpha: 0.25
      focal loss gamma: 2.0
  (label_enc): Embedding(80, 256)
)
[07/15 13:40:09] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 5.09 seconds.
[07/15 13:40:10] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/15 13:40:12] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/15 13:40:13] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/15 13:40:13] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/15 13:40:13] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/15 13:40:14] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/15 13:40:14] d2.data.build INFO: Making batched data loader with batch_size=16
[07/15 13:40:15] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/15 13:40:15] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/15 13:40:15] d2.engine.train_loop INFO: Starting training from iteration 0
[07/15 13:40:16] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/hawkeye/works/clear-vit/object_detection/dino/train_net.py", line 105, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/dino/modeling/dino.py", line 189, in forward
    features = self.backbone(images.tensor)  # output feature dict
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detrex/detrex/modeling/backbone/eva.py", line 583, in forward
    bottom_up_features = self.net(x)
                         ^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 285, in forward
    assert h % patch_size == 0 and w % patch_size == 0, "Image size must be divisible by patch size"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Image size must be divisible by patch size
[07/15 13:40:16] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[07/15 13:40:16] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 426M
[07/15 13:46:32] detectron2 INFO: Rank of current process: 0. World size: 1
[07/15 13:46:33] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hawkeye/works/clear-vit/object_detection/detectron2
Compiler                         GCC 13.3
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.7.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 5070 Ti (arch=12.0)
Driver version                   570.133.20
CUDA_HOME                        /usr
Pillow                           11.3.0
torchvision                      0.22.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120;-gencode;arch=compute_120,code=compute_120
  - CuDNN 90.7.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=12.8, CUDNN_VERSION=9.7.1, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[07/15 13:46:33] detectron2 INFO: Command line arguments: Namespace(config_file='configs/dino-vitdet/dino_vitdet_base_modified.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=[])
[07/15 13:46:33] detectron2 INFO: Contents of args.config_file=configs/dino-vitdet/dino_vitdet_base_modified.py:
[38;5;245m# train_simple_vit.py[39m

[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mdino_vitdet_modified[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;245m# Common defaults[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/coco_detr.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mdataloader[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mAdamW[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mlr_multiplier_12ep[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mtrain[39m

[38;5;245m# Custom training parameters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;245m# No pretrained weights[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/dino_simple_vit[39m[38;5;186m"[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mcheckpointer[39m[38;5;204m.[39m[38;5;15mperiod[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mmax_norm[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mnorm_type[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcuda[39m[38;5;186m"[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m

[38;5;245m# Optimizer config[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mbetas[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0.9[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m0.999[39m[38;5;15m)[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;204min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;245m# Dataloader config[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m

[07/15 13:46:33] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/dino_simple_vit/config.yaml is human-readable but cannot be loaded.
[07/15 13:46:33] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/dino_simple_vit/config.yaml.pkl.
[07/15 13:46:33] detectron2 INFO: Full config saved to ./output/dino_simple_vit/config.yaml
[07/15 13:46:33] d2.utils.env INFO: Using a generated random seed 34031454
[07/15 13:46:33] detectron2 INFO: Model:
DINO(
  (backbone): SimpleFeaturePyramid(
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): SimpleViT(
      (vit): SimpleVisionTransformer(
        (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (encoder): Encoder(
          (dropout): Dropout(p=0.1, inplace=False)
          (layers): Sequential(
            (encoder_layer_0): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_1): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_2): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_3): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_4): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_5): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_6): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_7): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_8): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_9): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_10): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_11): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (heads): Sequential(
          (head): Linear(in_features=768, out_features=0, bias=True)
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (position_embedding): PositionEmbeddingSine()
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0-3): 4 x ConvNormAct(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  (transformer): DINOTransformer(
    (encoder): DINOTransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DINOTransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (class_embed): ModuleList(
        (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-6): 7 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (class_embed): ModuleList(
    (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
  )
  (bbox_embed): ModuleList(
    (0-6): 7 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (criterion): Criterion DINOCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 1, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0}
      num_classes: 80
      eos_coef: None
      focal loss alpha: 0.25
      focal loss gamma: 2.0
  (label_enc): Embedding(80, 256)
)
[07/15 13:46:38] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 5.06 seconds.
[07/15 13:46:39] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/15 13:46:41] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/15 13:46:42] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/15 13:46:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/15 13:46:42] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/15 13:46:43] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/15 13:46:43] d2.data.build INFO: Making batched data loader with batch_size=16
[07/15 13:46:44] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/15 13:46:44] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/15 13:46:44] d2.engine.train_loop INFO: Starting training from iteration 0
[07/15 13:46:44] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/hawkeye/works/clear-vit/object_detection/dino/train_net.py", line 105, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/dino/modeling/dino.py", line 189, in forward
    features = self.backbone(images.tensor)  # output feature dict
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detrex/detrex/modeling/backbone/eva.py", line 583, in forward
    bottom_up_features = self.net(x)
                         ^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 285, in forward
    assert h % patch_size == 0 and w % patch_size == 0, "Image size must be divisible by patch size"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Image size must be divisible by patch size
[07/15 13:46:44] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[07/15 13:46:44] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 674M
[07/15 14:36:25] detectron2 INFO: Rank of current process: 0. World size: 1
[07/15 14:36:26] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hawkeye/works/clear-vit/object_detection/detectron2
Compiler                         GCC 13.3
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.7.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 5070 Ti (arch=12.0)
Driver version                   570.133.20
CUDA_HOME                        /usr
Pillow                           11.3.0
torchvision                      0.22.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120;-gencode;arch=compute_120,code=compute_120
  - CuDNN 90.7.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=12.8, CUDNN_VERSION=9.7.1, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[07/15 14:36:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/dino-vitdet/dino_vitdet_base_modified.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=[])
[07/15 14:36:26] detectron2 INFO: Contents of args.config_file=configs/dino-vitdet/dino_vitdet_base_modified.py:
[38;5;245m# train_simple_vit.py[39m

[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mdino_vitdet_modified[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;245m# Common defaults[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/coco_detr.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mdataloader[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mAdamW[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mlr_multiplier_12ep[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mtrain[39m

[38;5;245m# Custom training parameters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;245m# No pretrained weights[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/dino_simple_vit[39m[38;5;186m"[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mcheckpointer[39m[38;5;204m.[39m[38;5;15mperiod[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mmax_norm[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mnorm_type[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcuda[39m[38;5;186m"[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m

[38;5;245m# Optimizer config[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mbetas[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0.9[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m0.999[39m[38;5;15m)[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;204min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;245m# Dataloader config[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m

[07/15 14:36:26] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/dino_simple_vit/config.yaml is human-readable but cannot be loaded.
[07/15 14:36:26] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/dino_simple_vit/config.yaml.pkl.
[07/15 14:36:26] detectron2 INFO: Full config saved to ./output/dino_simple_vit/config.yaml
[07/15 14:36:26] d2.utils.env INFO: Using a generated random seed 27061016
[07/15 14:36:26] detectron2 INFO: Model:
DINO(
  (backbone): SimpleFeaturePyramid(
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): SimpleViT(
      (vit): SimpleVisionTransformer(
        (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (encoder): Encoder(
          (dropout): Dropout(p=0.1, inplace=False)
          (layers): Sequential(
            (encoder_layer_0): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_1): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_2): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_3): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_4): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_5): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_6): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_7): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_8): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_9): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_10): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_11): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (heads): Sequential(
          (head): Linear(in_features=768, out_features=0, bias=True)
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (position_embedding): PositionEmbeddingSine()
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0-3): 4 x ConvNormAct(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  (transformer): DINOTransformer(
    (encoder): DINOTransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DINOTransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (class_embed): ModuleList(
        (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-6): 7 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (class_embed): ModuleList(
    (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
  )
  (bbox_embed): ModuleList(
    (0-6): 7 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (criterion): Criterion DINOCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 1, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0}
      num_classes: 80
      eos_coef: None
      focal loss alpha: 0.25
      focal loss gamma: 2.0
  (label_enc): Embedding(80, 256)
)
[07/15 14:36:32] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 5.16 seconds.
[07/15 14:36:32] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/15 14:36:34] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/15 14:36:35] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/15 14:36:35] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/15 14:36:35] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/15 14:36:36] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/15 14:36:36] d2.data.build INFO: Making batched data loader with batch_size=16
[07/15 14:36:37] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/15 14:36:37] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/15 14:36:37] d2.engine.train_loop INFO: Starting training from iteration 0
[07/15 14:36:38] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/hawkeye/works/clear-vit/object_detection/dino/train_net.py", line 105, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/dino/modeling/dino.py", line 189, in forward
    features = self.backbone(images.tensor)  # output feature dict
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detrex/detrex/modeling/backbone/eva.py", line 583, in forward
    bottom_up_features = self.net(x)
                         ^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 289, in forward
    x = self.vit._process_input(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 189, in _process_input
    torch._assert(h == self.image_size, f"Wrong image height! Expected {self.image_size} but got {h}!")
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/__init__.py", line 2150, in _assert
    assert condition, message
           ^^^^^^^^^
AssertionError: Wrong image height! Expected 1024 but got 930!
[07/15 14:36:38] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[07/15 14:36:38] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 722M
[07/15 14:38:53] detectron2 INFO: Rank of current process: 0. World size: 1
[07/15 14:38:54] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hawkeye/works/clear-vit/object_detection/detectron2
Compiler                         GCC 13.3
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.7.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 5070 Ti (arch=12.0)
Driver version                   570.133.20
CUDA_HOME                        /usr
Pillow                           11.3.0
torchvision                      0.22.1+cu128 @/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120;-gencode;arch=compute_120,code=compute_120
  - CuDNN 90.7.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=12.8, CUDNN_VERSION=9.7.1, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[07/15 14:38:54] detectron2 INFO: Command line arguments: Namespace(config_file='configs/dino-vitdet/dino_vitdet_base_modified.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=[])
[07/15 14:38:54] detectron2 INFO: Contents of args.config_file=configs/dino-vitdet/dino_vitdet_base_modified.py:
[38;5;245m# train_simple_vit.py[39m

[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mdino_vitdet_modified[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;245m# Common defaults[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/coco_detr.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mdataloader[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mAdamW[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mlr_multiplier_12ep[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mtrain[39m

[38;5;245m# Custom training parameters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;245m# No pretrained weights[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/dino_simple_vit[39m[38;5;186m"[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mcheckpointer[39m[38;5;204m.[39m[38;5;15mperiod[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mmax_norm[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mnorm_type[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcuda[39m[38;5;186m"[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m

[38;5;245m# Optimizer config[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mbetas[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0.9[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m0.999[39m[38;5;15m)[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;204min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;245m# Dataloader config[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m

[07/15 14:38:54] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/dino_simple_vit/config.yaml is human-readable but cannot be loaded.
[07/15 14:38:54] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/dino_simple_vit/config.yaml.pkl.
[07/15 14:38:54] detectron2 INFO: Full config saved to ./output/dino_simple_vit/config.yaml
[07/15 14:38:54] d2.utils.env INFO: Using a generated random seed 54890666
[07/15 14:38:54] detectron2 INFO: Model:
DINO(
  (backbone): SimpleFeaturePyramid(
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): SimpleViT(
      (vit): SimpleVisionTransformer(
        (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (encoder): Encoder(
          (dropout): Dropout(p=0.1, inplace=False)
          (layers): Sequential(
            (encoder_layer_0): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_1): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_2): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_3): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_4): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_5): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_6): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_7): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_8): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_9): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_10): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
            (encoder_layer_11): EncoderBlock(
              (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (self_attention): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): MLPBlock(
                (linear_1): Linear(in_features=768, out_features=3072, bias=True)
                (activation): GELU(approximate='none')
                (dropout_1): Dropout(p=0.1, inplace=False)
                (linear_2): Linear(in_features=3072, out_features=768, bias=True)
                (dropout_2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (heads): Sequential(
          (head): Linear(in_features=768, out_features=0, bias=True)
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (position_embedding): PositionEmbeddingSine()
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0-3): 4 x ConvNormAct(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  (transformer): DINOTransformer(
    (encoder): DINOTransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoder): DINOTransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (class_embed): ModuleList(
        (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-6): 7 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (class_embed): ModuleList(
    (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
  )
  (bbox_embed): ModuleList(
    (0-6): 7 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (criterion): Criterion DINOCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 1, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0}
      num_classes: 80
      eos_coef: None
      focal loss alpha: 0.25
      focal loss gamma: 2.0
  (label_enc): Embedding(80, 256)
)
[07/15 14:38:59] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 5.08 seconds.
[07/15 14:39:00] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/15 14:39:02] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/15 14:39:03] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/15 14:39:03] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/15 14:39:03] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/15 14:39:04] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/15 14:39:04] d2.data.build INFO: Making batched data loader with batch_size=16
[07/15 14:39:05] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/15 14:39:05] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/15 14:39:05] d2.engine.train_loop INFO: Starting training from iteration 0
[07/15 14:39:05] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/hawkeye/works/clear-vit/object_detection/dino/train_net.py", line 105, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/dino/modeling/dino.py", line 189, in forward
    features = self.backbone(images.tensor)  # output feature dict
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/vit.py", line 489, in forward
    bottom_up_features = self.net(x)
                         ^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 289, in forward
    x = self.vit._process_input(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hawkeye/works/clear-vit/object_detection/detectron2/modeling/backbone/custom_vit.py", line 189, in _process_input
    torch._assert(h == self.image_size, f"Wrong image height! Expected {self.image_size} but got {h}!")
  File "/home/hawkeye/works/clear-vit/myenv/lib/python3.12/site-packages/torch/__init__.py", line 2150, in _assert
    assert condition, message
           ^^^^^^^^^
AssertionError: Wrong image height! Expected 1024 but got 800!
[07/15 14:39:05] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[07/15 14:39:05] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 675M
