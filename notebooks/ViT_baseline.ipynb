{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T16:55:04.110242Z",
     "iopub.status.busy": "2025-06-26T16:55:04.109795Z",
     "iopub.status.idle": "2025-06-26T16:55:13.775685Z",
     "shell.execute_reply": "2025-06-26T16:55:13.775045Z",
     "shell.execute_reply.started": "2025-06-26T16:55:04.110220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Necessary Imports and dependencies\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Any, Dict, Union, Type, Callable, Optional, List\n",
    "from torchvision.models.vision_transformer import MLPBlock\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageOps, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T16:55:13.777635Z",
     "iopub.status.busy": "2025-06-26T16:55:13.777053Z",
     "iopub.status.idle": "2025-06-26T16:55:13.782175Z",
     "shell.execute_reply": "2025-06-26T16:55:13.781386Z",
     "shell.execute_reply.started": "2025-06-26T16:55:13.777614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs and Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T16:55:13.783145Z",
     "iopub.status.busy": "2025-06-26T16:55:13.782912Z",
     "iopub.status.idle": "2025-06-26T16:55:13.887213Z",
     "shell.execute_reply": "2025-06-26T16:55:13.886453Z",
     "shell.execute_reply.started": "2025-06-26T16:55:13.783123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 90\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T16:55:14.330085Z",
     "iopub.status.busy": "2025-06-26T16:55:14.329764Z",
     "iopub.status.idle": "2025-06-26T16:55:19.421659Z",
     "shell.execute_reply": "2025-06-26T16:55:19.421099Z",
     "shell.execute_reply.started": "2025-06-26T16:55:14.330063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandAugment:\n",
    "    def __init__(self, n=9, m=0.5):\n",
    "        self.n = n\n",
    "        self.m = m  # [0, 30] in paper, but we use [0, 1] for simplicity\n",
    "        self.augment_list = [\n",
    "            self.auto_contrast, self.equalize, self.rotate, self.solarize, \n",
    "            self.color, self.contrast, self.brightness, self.sharpness,\n",
    "            self.shear_x, self.shear_y, self.translate_x, self.translate_y,\n",
    "            self.posterize, self.solarize_add, self.invert, self.identity\n",
    "        ]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_list, k=self.n)\n",
    "        for op in ops:\n",
    "            img = op(img)\n",
    "        return img\n",
    "\n",
    "    def auto_contrast(self, img):\n",
    "        return ImageOps.autocontrast(img)\n",
    "\n",
    "    def equalize(self, img):\n",
    "        return ImageOps.equalize(img)\n",
    "\n",
    "    def rotate(self, img):\n",
    "        return TF.rotate(img, self.m * 30)\n",
    "\n",
    "    def solarize(self, img):\n",
    "        return TF.solarize(img, int((1 - self.m) * 255))\n",
    "\n",
    "    def color(self, img):\n",
    "        return TF.adjust_saturation(img, 1 + self.m)\n",
    "\n",
    "    def contrast(self, img):\n",
    "        return TF.adjust_contrast(img, 1 + self.m)\n",
    "\n",
    "    def brightness(self, img):\n",
    "        return TF.adjust_brightness(img, 1 + self.m)\n",
    "\n",
    "    def sharpness(self, img):\n",
    "        return ImageEnhance.Sharpness(img).enhance(1 + self.m)\n",
    "\n",
    "    def shear_x(self, img):\n",
    "        return TF.affine(img, 0, [0, 0], 1, [self.m, 0])\n",
    "\n",
    "    def shear_y(self, img):\n",
    "        return TF.affine(img, 0, [0, 0], 1, [0, self.m])\n",
    "\n",
    "    def translate_x(self, img):\n",
    "        return TF.affine(img, 0, [int(self.m * img.size[0] / 3), 0], 1, [0, 0])\n",
    "\n",
    "    def translate_y(self, img):\n",
    "        return TF.affine(img, 0, [0, int(self.m * img.size[1] / 3)], 1, [0, 0])\n",
    "\n",
    "    def posterize(self, img):\n",
    "        return TF.posterize(img, int((1 - self.m) * 8))\n",
    "\n",
    "    def solarize_add(self, img):\n",
    "        return TF.solarize(TF.adjust_brightness(img, 1 + self.m), int((1 - self.m) * 255))\n",
    "\n",
    "    def invert(self, img):\n",
    "        return TF.invert(img) if random.random() < 0.5 else img\n",
    "\n",
    "    def identity(self, img):\n",
    "        return img\n",
    "\n",
    "class Mixup(nn.Module):\n",
    "    def __init__(self, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, batch):\n",
    "        images, labels = batch\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = images.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "        labels_a, labels_b = labels, labels[index]\n",
    "        return mixed_images, labels_a, labels_b, lam\n",
    "\n",
    "class CutMix(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, batch):\n",
    "        images, labels = batch\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size, _, H, W = images.shape\n",
    "        cx = np.random.uniform(0, W)\n",
    "        cy = np.random.uniform(0, H)\n",
    "        w = W * np.sqrt(1 - lam)\n",
    "        h = H * np.sqrt(1 - lam)\n",
    "        x0 = int(np.clip(cx - w // 2, 0, W))\n",
    "        y0 = int(np.clip(cy - h // 2, 0, H))\n",
    "        x1 = int(np.clip(cx + w // 2, 0, W))\n",
    "        y1 = int(np.clip(cy + h // 2, 0, H))\n",
    "        index = torch.randperm(batch_size)\n",
    "        images[:, :, y0:y1, x0:x1] = images[index, :, y0:y1, x0:x1]\n",
    "        lam = 1 - ((x1 - x0) * (y1 - y0) / (W * H))\n",
    "        labels_a, labels_b = labels, labels[index]\n",
    "        return images, labels_a, labels_b, lam\n",
    "\n",
    "class RandomErasing(nn.Module):\n",
    "    def __init__(self, probability=0.25, sl=0.02, sh=0.4, r1=0.3, r2=1/0.3):\n",
    "        super().__init__()\n",
    "        self.probability = probability\n",
    "        self.sl = sl\n",
    "        self.sh = sh\n",
    "        self.r1 = r1\n",
    "        self.r2 = r2\n",
    "\n",
    "    def forward(self, img):\n",
    "        if random.uniform(0, 1) > self.probability:\n",
    "            return img\n",
    "        \n",
    "        for attempt in range(100):\n",
    "            area = img.size()[1] * img.size()[2]\n",
    "            target_area = random.uniform(self.sl, self.sh) * area\n",
    "            aspect_ratio = random.uniform(self.r1, self.r2)\n",
    "\n",
    "            h = int(round(np.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(np.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if w < img.size()[2] and h < img.size()[1]:\n",
    "                x1 = random.randint(0, img.size()[1] - h)\n",
    "                y1 = random.randint(0, img.size()[2] - w)\n",
    "                if img.size()[0] == 3:\n",
    "                    img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    img[1, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    img[2, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                else:\n",
    "                    img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                return img\n",
    "        return img\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
    "        smooth_one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_classes\n",
    "        log_prob = nn.functional.log_softmax(pred, dim=1)\n",
    "        return torch.mean(torch.sum(-smooth_one_hot * log_prob, dim=1))\n",
    "\n",
    "# Updated ImageNet100Dataset\n",
    "class ImageNet100Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dirs, labels_file, transform=None, augment=None):\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.label_to_idx = {}\n",
    "        \n",
    "        with open(labels_file, 'r') as f:\n",
    "            label_dict = json.load(f)\n",
    "        \n",
    "        unique_labels = sorted(label_dict.keys())\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        \n",
    "        for root_dir in root_dirs:\n",
    "            for label in os.listdir(root_dir):\n",
    "                label_path = os.path.join(root_dir, label)\n",
    "                if os.path.isdir(label_path):\n",
    "                    for img_name in os.listdir(label_path):\n",
    "                        img_path = os.path.join(label_path, img_name)\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.label_to_idx[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.augment:\n",
    "            image = self.augment(image)\n",
    "        \n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256, scale=(0.08, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(n=3, m=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    RandomErasing(probability=0.25)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create the datasets\n",
    "train_dirs = [\n",
    "    '../data/train.X1',\n",
    "    '../data/train.X2',\n",
    "    '../data/train.X3',\n",
    "    '../data/train.X4'\n",
    "]\n",
    "val_dir = ['../data/val.X']\n",
    "labels_file = '../data/Labels.json'\n",
    "\n",
    "train_dataset = ImageNet100Dataset(\n",
    "    root_dirs=train_dirs,\n",
    "    labels_file=labels_file,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageNet100Dataset(\n",
    "    root_dirs=val_dir,\n",
    "    labels_file=labels_file,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Custom collate function for Mixup and CutMix\n",
    "def collate_fn(batch):\n",
    "    images, labels = torch.utils.data.default_collate(batch)\n",
    "    if random.random() < 0.5:\n",
    "        return Mixup(alpha=0.8)((images, labels))\n",
    "    else:\n",
    "        return CutMix(alpha=1.0)((images, labels))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T16:55:20.385944Z",
     "iopub.status.busy": "2025-06-26T16:55:20.385330Z",
     "iopub.status.idle": "2025-06-26T16:55:20.389481Z",
     "shell.execute_reply": "2025-06-26T16:55:20.388774Z",
     "shell.execute_reply.started": "2025-06-26T16:55:20.385918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n = len(train_dataset)\n",
    "total_steps = round((n * num_epochs) / batch_size)\n",
    "warmup_try = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T16:55:20.995027Z",
     "iopub.status.busy": "2025-06-26T16:55:20.994755Z",
     "iopub.status.idle": "2025-06-26T16:55:21.023926Z",
     "shell.execute_reply": "2025-06-26T16:55:21.023261Z",
     "shell.execute_reply.started": "2025-06-26T16:55:20.995005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_dim, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_dim, mlp_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(mlp_dim, in_dim)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "        # Fix init discrepancy between nn.MultiheadAttention and that of big_vision\n",
    "        bound = math.sqrt(3 / hidden_dim)\n",
    "        nn.init.uniform_(self.self_attention.in_proj_weight, -bound, bound)\n",
    "        nn.init.uniform_(self.self_attention.out_proj.weight, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        return self.ln(self.layers(self.dropout(input)))\n",
    "\n",
    "\n",
    "def jax_lecun_normal(layer, fan_in):\n",
    "    \"\"\"(re-)initializes layer weight in the same way as jax.nn.initializers.lecun_normal and bias to zero\"\"\"\n",
    "\n",
    "    # constant is stddev of standard normal truncated to (-2, 2)\n",
    "    std = math.sqrt(1 / fan_in) / .87962566103423978\n",
    "    nn.init.trunc_normal_(layer.weight, std=std, a=-2 * std, b=2 * std)\n",
    "    if layer.bias is not None:\n",
    "        nn.init.zeros_(layer.bias)\n",
    "        \n",
    "class SimpleVisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer modified per https://arxiv.org/abs/2205.01580.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 100,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        h = w = image_size // patch_size \n",
    "\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_length, hidden_dim) * 0.02, requires_grad=True)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02, requires_grad=True)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            norm_layer,\n",
    "        )\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "        \n",
    "        # Init the patchify stem\n",
    "        fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1] // self.conv_proj.groups\n",
    "        jax_lecun_normal(self.conv_proj, fan_in)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            jax_lecun_normal(self.heads.pre_logits, fan_in)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # Prepend the CLS token\n",
    "        cls_token = self.cls_token.expand(n, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "        # Pass through the encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Use only the CLS token for classification\n",
    "        x = x[:, 0]\n",
    "\n",
    "        # Pass through the classification head\n",
    "        x = self.heads(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:02:39.929219Z",
     "iopub.status.busy": "2025-06-26T17:02:39.928538Z",
     "iopub.status.idle": "2025-06-26T17:02:40.009068Z",
     "shell.execute_reply": "2025-06-26T17:02:40.008555Z",
     "shell.execute_reply.started": "2025-06-26T17:02:39.929194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def weight_decay_param(n, p):\n",
    "    return p.ndim >= 2 and n.endswith('weight')\n",
    "\n",
    "# create model\n",
    "model = SimpleVisionTransformer(\n",
    "    image_size=256,\n",
    "    patch_size=16,\n",
    "    num_layers=12,\n",
    "    num_heads=3,\n",
    "    hidden_dim=192,\n",
    "    mlp_dim=768,\n",
    ")\n",
    "model = nn.DataParallel(model)\n",
    "model.to('cuda')\n",
    "\n",
    "wd_params = [p for n, p in model.named_parameters() if weight_decay_param(n, p) and p.requires_grad]\n",
    "non_wd_params = [p for n, p in model.named_parameters() if not weight_decay_param(n, p) and p.requires_grad]\n",
    "original_model = model\n",
    "\n",
    "weight_decay = 0.05\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Label smoothing loss\n",
    "criterion = LabelSmoothing(smoothing=0.1)\n",
    "criterion.to(device)\n",
    "# criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": wd_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": non_wd_params, \"weight_decay\": weight_decay},\n",
    "    ],\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999)  # Set beta1=0.9 and beta2=0.999\n",
    ")\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: step / warmup_try)\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps - warmup_try)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [warmup, cosine], [warmup_try])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:02:42.575566Z",
     "iopub.status.busy": "2025-06-26T17:02:42.574882Z",
     "iopub.status.idle": "2025-06-26T17:02:42.581736Z",
     "shell.execute_reply": "2025-06-26T17:02:42.581160Z",
     "shell.execute_reply.started": "2025-06-26T17:02:42.575535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Parameters': 5555044,\n",
       " 'Trainable Parameters': 5555044,\n",
       " 'Non-trainable Parameters': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"Total Parameters\": total_params,\n",
    "        \"Trainable Parameters\": trainable_params,\n",
    "        \"Non-trainable Parameters\": total_params - trainable_params\n",
    "    }\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:02:46.152445Z",
     "iopub.status.busy": "2025-06-26T17:02:46.151897Z",
     "iopub.status.idle": "2025-06-26T17:02:46.164668Z",
     "shell.execute_reply": "2025-06-26T17:02:46.163832Z",
     "shell.execute_reply.started": "2025-06-26T17:02:46.152422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Change_path_for_the_directory;This is the directory where model weights are to be saved\n",
    "checkpoint_path = \"../checkpoints\"\n",
    "\n",
    "def save_checkpoint(state, is_best, path, filename='imagenet_baseline_patchconvcheckpoint.pth.tar'):\n",
    "    filename = os.path.join(path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(path, 'model_best.pth.tar'))\n",
    "\n",
    "def save_checkpoint_step(step, model, best_acc1, optimizer, scheduler, checkpoint_path):\n",
    "    # Define the filename with the current step\n",
    "    filename = os.path.join(checkpoint_path, f'BaseLine_VIT.pt')\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }, filename)\n",
    "    \n",
    "\n",
    "class Summary(Enum):\n",
    "    NONE = 0\n",
    "    AVERAGE = 1\n",
    "    SUM = 2\n",
    "    COUNT = 3\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.summary_type = summary_type\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def all_reduce(self):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)\n",
    "        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n",
    "        self.sum, self.count = total.tolist()\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "    def summary(self):\n",
    "        fmtstr = ''\n",
    "        if self.summary_type is Summary.NONE:\n",
    "            fmtstr = ''\n",
    "        elif self.summary_type is Summary.AVERAGE:\n",
    "            fmtstr = '{name} {avg:.3f}'\n",
    "        elif self.summary_type is Summary.SUM:\n",
    "            fmtstr = '{name} {sum:.3f}'\n",
    "        elif self.summary_type is Summary.COUNT:\n",
    "            fmtstr = '{name} {count:.3f}'\n",
    "        else:\n",
    "            raise ValueError('invalid summary type %r' % self.summary_type)\n",
    "        \n",
    "        return fmtstr.format(**self.__dict__)\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "        \n",
    "    def display_summary(self):\n",
    "        entries = [\" *\"]\n",
    "        entries += [meter.summary() for meter in self.meters]\n",
    "        print(' '.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:02:46.515988Z",
     "iopub.status.busy": "2025-06-26T17:02:46.515758Z",
     "iopub.status.idle": "2025-06-26T17:02:46.519587Z",
     "shell.execute_reply": "2025-06-26T17:02:46.518831Z",
     "shell.execute_reply.started": "2025-06-26T17:02:46.515970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "log_steps = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:02:47.118720Z",
     "iopub.status.busy": "2025-06-26T17:02:47.117950Z",
     "iopub.status.idle": "2025-06-26T17:02:47.122199Z",
     "shell.execute_reply": "2025-06-26T17:02:47.121649Z",
     "shell.execute_reply.started": "2025-06-26T17:02:47.118692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:02:49.217238Z",
     "iopub.status.busy": "2025-06-26T17:02:49.216950Z",
     "iopub.status.idle": "2025-06-26T17:02:52.580195Z",
     "shell.execute_reply": "2025-06-26T17:02:52.579225Z",
     "shell.execute_reply.started": "2025-06-26T17:02:49.217216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  100/45703]\tTime  1.760 ( 1.731)\tData  1.406 ( 1.368)\tLoss 4.6047e+00 (4.6050e+00)\tAcc@1   2.63 (  1.80)\tAcc@5   8.74 (  8.31)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 157\u001b[39m\n\u001b[32m    154\u001b[39m         scheduler.step()\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Use the modified train function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device)\u001b[39m\n\u001b[32m    104\u001b[39m acc1 = lam * acc1_a + (\u001b[32m1\u001b[39m - lam) * acc1_b\n\u001b[32m    105\u001b[39m acc5 = lam * acc5_a + (\u001b[32m1\u001b[39m - lam) * acc5_b\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m losses.update(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, images.size(\u001b[32m0\u001b[39m))\n\u001b[32m    108\u001b[39m top1.update(acc1[\u001b[32m0\u001b[39m].item(), images.size(\u001b[32m0\u001b[39m))\n\u001b[32m    109\u001b[39m top5.update(acc5[\u001b[32m0\u001b[39m].item(), images.size(\u001b[32m0\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def validate(val_loader, model, criterion, step, use_wandb=False, print_freq=100):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda(non_blocking=True)\n",
    "                target = target.cuda(non_blocking=True)\n",
    "            elif torch.backends.mps.is_available():\n",
    "                images = images.to('mps')\n",
    "                target = target.to('mps')\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "    progress.display_summary()\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def train(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device):\n",
    "    \n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    print_freq = 100\n",
    "    log_steps = 2500\n",
    "\n",
    "    best_acc1 = 0.0\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        total_steps,\n",
    "        [batch_time, data_time, losses, top1, top5]\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    \n",
    "    def infinite_loader():\n",
    "        while True:\n",
    "            yield from train_loader\n",
    "            \n",
    "    for step, (images, labels_a, labels_b, lam) in zip(range(start_step + 1, total_steps + 1), infinite_loader()):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels_a = labels_a.to(device, non_blocking=True)\n",
    "        labels_b = labels_b.to(device, non_blocking=True)\n",
    "        \n",
    "        # Convert lam to a tensor if it's not already one\n",
    "        if not isinstance(lam, torch.Tensor):\n",
    "            lam = torch.tensor(lam, device=device)\n",
    "        else:\n",
    "            lam = lam.to(device, non_blocking=True)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = lam * criterion(output, labels_a) + (1 - lam) * criterion(output, labels_b)\n",
    "\n",
    "        # Compute accuracy (this is an approximation for mixed labels)\n",
    "        acc1_a, acc5_a = accuracy(output, labels_a, topk=(1, 5))\n",
    "        acc1_b, acc5_b = accuracy(output, labels_b, topk=(1, 5))\n",
    "        acc1 = lam * acc1_a + (1 - lam) * acc1_b\n",
    "        acc5 = lam * acc5_a + (1 - lam) * acc5_b\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0].item(), images.size(0))\n",
    "        top5.update(acc5[0].item(), images.size(0))\n",
    "\n",
    "        loss.backward()\n",
    "        l2_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % print_freq == 0:\n",
    "            progress.display(step)\n",
    "        \n",
    "        if ((step % print_freq == 0) and ((step % log_steps != 0) and (step != total_steps))):        \n",
    "            save_checkpoint_step(step, model, best_acc1, optimizer, scheduler, checkpoint_path)\n",
    "                \n",
    "        if step % log_steps == 0:\n",
    "            acc1 = validate(val_loader, original_model, criterion, step)\n",
    "            is_best = acc1 > best_acc1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            \n",
    "            save_checkpoint({\n",
    "                'step': step,\n",
    "                'state_dict': original_model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'scheduler' : scheduler.state_dict()\n",
    "            }, is_best, checkpoint_path)\n",
    "            \n",
    "        elif step == total_steps:\n",
    "            acc1 = validate(val_loader, original_model, criterion, step)\n",
    "            is_best = acc1 > best_acc1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            \n",
    "            save_checkpoint({\n",
    "                'step': step,\n",
    "                'state_dict': original_model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'scheduler' : scheduler.state_dict()\n",
    "            }, is_best, checkpoint_path)\n",
    "            \n",
    "        if step % 27000 == 0 and step > 0:\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "# Use the modified train function\n",
    "train(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1500837,
     "sourceId": 2491748,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
